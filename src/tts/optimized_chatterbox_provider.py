import torch
import numpy as np
import librosa
import tempfile
import os
from typing import Optional, Dict, Any, Generator
from contextlib import contextmanager
import functools
import time

try:
    from chatterbox.tts import ChatterboxTTS
    CHATTERBOX_AVAILABLE = True
except ImportError:
    CHATTERBOX_AVAILABLE = False
    ChatterboxTTS = None

class OptimizedChatterboxProvider:
    """
    Optimized ChatterboxTTS Provider vá»›i cÃ¡c cáº£i tiáº¿n tá»‘c Ä‘á»™:
    - Torch compilation vá»›i CUDA graphs (2-4x faster)
    - Mixed precision (float16/float32) 
    - Voice embedding caching
    - Chunked processing vá»›i streaming
    - CPU offloading for memory management
    """
    
    def __init__(self, device: str = "cuda", dtype: str = "float16", use_compilation: bool = True, cpu_offload: bool = False):
        self.device = self._resolve_device(device)
        self.dtype = self._resolve_dtype(dtype)
        self.use_compilation = use_compilation
        self.cpu_offload = cpu_offload
        self.model: Optional[ChatterboxTTS] = None
        self.voice_cache: Dict[str, Any] = {}
        self.compilation_enabled = False
        
        print(f"ðŸš€ OptimizedChatterboxProvider initialized:")
        print(f"   ðŸ“± Device: {self.device}")
        print(f"   ðŸŽ¯ Dtype: {self.dtype}")
        print(f"   âš¡ Compilation: {self.use_compilation}")
        print(f"   ðŸ’¾ CPU Offload: {self.cpu_offload}")
    
    def _resolve_device(self, device: str) -> str:
        """Auto-detect best device"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device
    
    def _resolve_dtype(self, dtype: str) -> torch.dtype:
        """Convert string dtype to torch dtype"""
        dtype_map = {
            "float32": torch.float32,
            "float16": torch.float16,
            "bfloat16": torch.bfloat16,
        }
        return dtype_map.get(dtype, torch.float32)
    
    def _optimize_model_precision(self, model: ChatterboxTTS) -> ChatterboxTTS:
        """Apply mixed precision optimizations"""
        print(f"ðŸŽ¯ Applying mixed precision optimization ({self.dtype})")
        
        # Voice Encoder - always float32 for stability
        model.ve.to(device=self.device, dtype=torch.float32)
        
        # T3 Model - can use mixed precision
        model.t3.to(device=self.device, dtype=self.dtype)
        if model.conds:
            model.conds.t3.to(device=self.device, dtype=self.dtype)
        
        # S3Gen - careful mixed precision
        if self.dtype == torch.float16:
            model.s3gen.flow.fp16 = True
        elif self.dtype == torch.float32:
            model.s3gen.flow.fp16 = False
            
        model.s3gen.to(device=self.device, dtype=self.dtype)
        
        # Keep these components in float32 for stability
        model.s3gen.mel2wav.to(dtype=torch.float32)
        model.s3gen.tokenizer.to(dtype=torch.float32) 
        model.s3gen.speaker_encoder.to(dtype=torch.float32)
        
        model.device = self.device
        torch.cuda.empty_cache()
        return model
    
    def _setup_compilation(self, model: ChatterboxTTS):
        """Setup torch compilation for T3 model"""
        if not self.use_compilation or self.device == "cpu":
            return
            
        try:
            print("âš¡ Setting up T3 compilation with CUDA graphs...")
            if not hasattr(model.t3, "_step_compilation_target_original"):
                model.t3._step_compilation_target_original = model.t3._step_compilation_target
            
            model.t3._step_compilation_target = torch.compile(
                model.t3._step_compilation_target,
                fullgraph=True,
                backend="cudagraphs"
            )
            
            # Warm up compilation with dummy run
            print("ðŸ”¥ Warming up compilation...")
            for i in range(2):
                print(f"   Warmup {i+1}/2")
                list(model.generate("Warmup compilation run"))
            
            self.compilation_enabled = True
            print("âœ… T3 compilation setup complete!")
            
        except Exception as e:
            print(f"âš ï¸ Compilation setup failed: {e}")
            self.compilation_enabled = False
    
    def _remove_compilation(self, model: ChatterboxTTS):
        """Remove torch compilation"""
        if hasattr(model.t3, "_step_compilation_target_original"):
            model.t3._step_compilation_target = model.t3._step_compilation_target_original
            self.compilation_enabled = False
            print("ðŸ”„ T3 compilation removed")
    
    @contextmanager
    def _cpu_offload_context(self, model: ChatterboxTTS):
        """Context manager for CPU offloading"""
        if self.cpu_offload:
            print("ðŸ“¤ Moving model to GPU...")
            self._optimize_model_precision(model)
        
        try:
            yield model
        finally:
            if self.cpu_offload:
                print("ðŸ“¥ Offloading model to CPU...")
                model.ve.to(device="cpu")
                model.t3.to(device="cpu")
                model.s3gen.to(device="cpu")
                if model.conds:
                    model.conds.to("cpu")
                torch.cuda.empty_cache()
    
    def load_model(self) -> bool:
        """Load vÃ  optimize ChatterboxTTS model"""
        if not CHATTERBOX_AVAILABLE:
            print("âŒ ChatterboxTTS not available")
            return False
        
        try:
            print("ðŸ”„ Loading ChatterboxTTS model...")
            self.model = ChatterboxTTS.from_pretrained(device=self.device)
            
            # Apply optimizations
            self.model = self._optimize_model_precision(self.model)
            
            # Setup compilation if enabled
            if self.use_compilation:
                self._setup_compilation(self.model)
            
            # Move to CPU for offloading
            if self.cpu_offload:
                print("ðŸ“¥ Moving model to CPU for offloading...")
                self.model.ve.to(device="cpu")
                self.model.t3.to(device="cpu") 
                self.model.s3gen.to(device="cpu")
                if self.model.conds:
                    self.model.conds.to("cpu")
            
            print("âœ… OptimizedChatterboxTTS loaded successfully!")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load ChatterboxTTS: {e}")
            return False
    
    def prepare_voice_conditionals(self, voice_path: str, exaggeration: float = 0.5) -> str:
        """Prepare vÃ  cache voice conditionals"""
        cache_key = f"{voice_path}_{exaggeration}"
        
        if cache_key in self.voice_cache:
            print(f"ðŸŽ¯ Using cached voice conditionals: {os.path.basename(voice_path)}")
            return cache_key
        
        try:
            print(f"ðŸŽ¤ Preparing voice conditionals: {os.path.basename(voice_path)}")
            self.model.prepare_conditionals(voice_path, exaggeration=exaggeration)
            
            # Convert to target dtype if needed
            if self.dtype != torch.float32 and self.model.conds:
                self.model.conds.t3.to(dtype=self.dtype)
            
            # Cache the conditionals
            self.voice_cache[cache_key] = {
                'conds': self.model.conds,
                'timestamp': time.time()
            }
            
            print(f"âœ… Voice conditionals cached: {cache_key}")
            return cache_key
            
        except Exception as e:
            print(f"âŒ Failed to prepare voice conditionals: {e}")
            return None
    
    def restore_cached_conditionals(self, cache_key: str):
        """Restore cached voice conditionals"""
        if cache_key in self.voice_cache:
            self.model.conds = self.voice_cache[cache_key]['conds']
            print(f"ðŸ”„ Restored cached conditionals: {cache_key}")
    
    def generate_optimized(
        self,
        text: str,
        voice_path: str,
        exaggeration: float = 0.5,
        cfg_weight: float = 0.5,
        temperature: float = 0.8,
        chunked: bool = False,
        chunk_size: int = 200,
        max_new_tokens: int = 1000,
        streaming: bool = False
    ) -> Generator[np.ndarray, None, None]:
        """
        Generate audio vá»›i optimizations
        """
        if not self.model:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        
        # Prepare voice conditionals (with caching)
        cache_key = self.prepare_voice_conditionals(voice_path, exaggeration)
        if not cache_key:
            raise RuntimeError("Failed to prepare voice conditionals")
        
        with self._cpu_offload_context(self.model):
            with torch.no_grad():
                try:
                    if chunked:
                        # Split text into chunks for large texts
                        from tts_webui.utils.split_text_functions import split_and_recombine_text
                        texts = split_and_recombine_text(text, chunk_size, chunk_size + 100)
                    else:
                        texts = [text]
                    
                    for i, chunk_text in enumerate(texts):
                        print(f"ðŸŽ¤ Generating chunk {i+1}/{len(texts)}: {chunk_text[:50]}...")
                        
                        # Restore cached conditionals for each chunk
                        self.restore_cached_conditionals(cache_key)
                        
                        # Generate audio for chunk
                        start_time = time.time()
                        wav = self.model.generate(
                            chunk_text,
                            exaggeration=exaggeration,
                            cfg_weight=cfg_weight,
                            temperature=temperature,
                            max_new_tokens=max_new_tokens
                        )
                        generation_time = time.time() - start_time
                        
                        # Convert to numpy
                        audio_np = wav.squeeze().cpu().numpy()
                        audio_duration = len(audio_np) / self.model.sr
                        
                        print(f"âœ… Chunk generated: {generation_time:.2f}s for {audio_duration:.2f}s audio")
                        print(f"   ðŸš€ Real-time factor: {audio_duration/generation_time:.2f}x")
                        
                        if streaming:
                            yield audio_np
                        else:
                            # For non-streaming, collect all chunks
                            if i == 0:
                                full_audio = audio_np
                            else:
                                full_audio = np.concatenate([full_audio, audio_np])
                    
                    if not streaming:
                        yield full_audio
                        
                except Exception as e:
                    print(f"âŒ Generation failed: {e}")
                    raise
    
    def generate(
        self,
        text: str,
        voice_path: str,
        emotion: str = "neutral",
        speed: float = 1.0,
        cfg_weight: float = 0.5,
        exaggeration: float = 0.5,
        temperature: float = 0.8,
        output_path: str = None,
        chunked: bool = False
    ) -> Optional[str]:
        """
        Main generation method vá»›i optimizations
        """
        if not self.model:
            if not self.load_model():
                return None
        
        try:
            # Convert emotion to exaggeration value
            emotion_map = {
                "neutral": 0.5,
                "happy": 0.7,
                "sad": 0.3,
                "angry": 0.9,
                "excited": 0.8,
                "calm": 0.2,
                "dramatic": 1.2,
                "whisper": 0.1
            }
            mapped_exaggeration = emotion_map.get(emotion, exaggeration)
            
            with self._cpu_offload_context(self.model):
                with torch.no_grad():
                    start_time = time.time()
                    
                    # Generate audio
                    wav = self.model.generate(
                        text,
                        audio_prompt_path=voice_path,
                        exaggeration=mapped_exaggeration,
                        cfg_weight=cfg_weight,
                        temperature=temperature
                    )
                    
                    generation_time = time.time() - start_time
                    
                    # Convert to numpy
                    audio_np = wav.squeeze().cpu().numpy()
                    audio_duration = len(audio_np) / self.model.sr
                    
                    print(f"âœ… Generated: {generation_time:.2f}s for {audio_duration:.2f}s audio")
                    print(f"   ðŸš€ Real-time factor: {audio_duration/generation_time:.2f}x")
                    
                    # Save audio
                    if output_path:
                        import soundfile as sf
                        sf.write(output_path, audio_np, self.model.sr)
                        print(f"ðŸ’¾ Audio saved: {output_path}")
                        return output_path
                    else:
                        # Save to temp file
                        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                            import soundfile as sf
                            sf.write(tmp.name, audio_np, self.model.sr)
                            return tmp.name
                            
        except Exception as e:
            print(f"âŒ OptimizedChatterbox generation failed: {e}")
            return None
    
    def clear_cache(self):
        """Clear voice cache"""
        self.voice_cache.clear()
        print("ðŸ—‘ï¸ Voice cache cleared")
    
    def get_status(self) -> Dict[str, Any]:
        """Get provider status"""
        return {
            "available": CHATTERBOX_AVAILABLE and (self.model is not None),
            "initialized": self.model is not None,
            "compilation_enabled": self.compilation_enabled,
            "device": str(self.device),
            "dtype": str(self.dtype),
            "cached_voices": len(self.voice_cache),
            "provider_type": "Optimized Chatterbox TTS (2-4x faster)"
        } 